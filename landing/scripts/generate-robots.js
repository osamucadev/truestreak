const fs = require("fs");
const path = require("path");

const publicDir = path.join(__dirname, "..", "public");
const configPath = path.join(__dirname, "..", "config.json");

// Carregar configura√ß√£o
let config = {};
if (fs.existsSync(configPath)) {
  config = JSON.parse(fs.readFileSync(configPath, "utf-8"));
}

const baseUrl = config.site?.url || "https://truestreak.life";

function generateRobots() {
  console.log("ü§ñ Gerando robots.txt...");

  // Template do robots.txt
  const robots = `# robots.txt gerado automaticamente
# ${new Date().toISOString()}

User-agent: *
Allow: /

# Sitemap
Sitemap: ${baseUrl}/sitemap.xml

# Crawl-delay (opcional - descomente se necess√°rio)
# Crawl-delay: 1

# Bloquear pastas espec√≠ficas (exemplo)
# Disallow: /admin/
# Disallow: /private/
`;

  // Salvar no public/
  const outputPath = path.join(publicDir, "robots.txt");
  fs.writeFileSync(outputPath, robots);

  console.log(`‚úì robots.txt gerado`);
  console.log(`  Sitemap: ${baseUrl}/sitemap.xml\n`);
}

generateRobots();
